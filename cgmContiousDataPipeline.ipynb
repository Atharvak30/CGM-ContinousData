{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, pdb, copy\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_time_series_data(df, cgm_cols=[\"Dexcom GL\", \"Libre GL\"], activity_cols=[\"HR\", \"METs\"]):\n",
    "    \"\"\"Clean and validate time-series data\"\"\"\n",
    "    # Copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    for col in cgm_cols + activity_cols:\n",
    "        if col in df.columns:\n",
    "            # Linear interpolation with 5-minute window\n",
    "            df[col] = df[col].interpolate(method='linear', limit=5)\n",
    "            # Forward/backward fill remaining\n",
    "            df[col] = df[col].ffill().bfill()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_daily_traces(\n",
    "    dataset_df: pd.DataFrame, \n",
    "    subject_id: int,\n",
    "    cgm_cols=[\"Dexcom GL\", \"Libre GL\"],\n",
    "    activity_cols=[\"HR\", \"METs\"],\n",
    "    img_size=(112, 112),\n",
    "    start_hour=6  # Start at 6 AM\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced version with full-day (1440 minute) time features.\n",
    "    \"\"\"\n",
    "    # Cleaning functions remain the same\n",
    "    def clean_series(series):\n",
    "        series = series.interpolate(method='linear', limit=5).ffill().bfill()\n",
    "        try:\n",
    "            from scipy.signal import savgol_filter\n",
    "            series = savgol_filter(series, window_length=15, polyorder=2)\n",
    "        except ImportError:\n",
    "            series = series.rolling(window=15, min_periods=1, center=True).mean()\n",
    "        return series\n",
    "\n",
    "    # Apply cleaning\n",
    "    cleaned_df = dataset_df.copy()\n",
    "    for col in cgm_cols + activity_cols:\n",
    "        if col in cleaned_df.columns:\n",
    "            cleaned_df[col] = clean_series(cleaned_df[col])\n",
    "    \n",
    "    # Resample to 1-minute frequency\n",
    "    resampled_df = cleaned_df.resample('1min').ffill(limit=5)\n",
    "    \n",
    "    # Fill NaNs with median\n",
    "    for col in cgm_cols + activity_cols:\n",
    "        if col in resampled_df.columns:\n",
    "            resampled_df[col] = resampled_df[col].fillna(resampled_df[col].median())\n",
    "    \n",
    "    # Statistics for normalization\n",
    "    cgm_stats = {\n",
    "        'mean': resampled_df[cgm_cols].mean().values if all(col in resampled_df.columns for col in cgm_cols) else np.zeros(len(cgm_cols)),\n",
    "        'std': resampled_df[cgm_cols].std().values if all(col in resampled_df.columns for col in cgm_cols) else np.ones(len(cgm_cols))\n",
    "    }\n",
    "    activity_stats = {\n",
    "        'mean': resampled_df[activity_cols].mean().values if all(col in resampled_df.columns for col in activity_cols) else np.zeros(len(activity_cols)),\n",
    "        'std': resampled_df[activity_cols].std().values if all(col in resampled_df.columns for col in activity_cols) else np.ones(len(activity_cols))\n",
    "    }\n",
    "\n",
    "    # Initialize arrays and dictionaries\n",
    "    days = pd.Series(resampled_df.index.date).unique()\n",
    "    days_list = [str(day) for day in days]\n",
    "    cgm_daily_data = np.full((len(days), len(cgm_cols), 1440), np.nan)\n",
    "    activity_daily_data = np.full((len(days), len(activity_cols), 1440), np.nan)\n",
    "    image_data_by_day = {}\n",
    "    nutrition_data_by_day = {}\n",
    "    timestamp_vectors = {}  # Store full day timestamps\n",
    "    meal_timing_features = {}  # Store meal timing features for full day\n",
    "    \n",
    "    # Time window parameters\n",
    "    minutes_after_last_meal = 6 * 60  # 6 hours after last meal\n",
    "\n",
    "    # Process each day\n",
    "    for i, day in enumerate(days):\n",
    "        day_start = pd.Timestamp(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1) - pd.Timedelta(minutes=1)\n",
    "        day_data = resampled_df.loc[day_start:day_end]\n",
    "        \n",
    "        # Extract meal information for this day\n",
    "        meal_rows = dataset_df.loc[day_start:day_end].dropna(subset=['Meal Type'])\n",
    "        meal_times = meal_rows.index.sort_values() if not meal_rows.empty else []\n",
    "        \n",
    "        # Create full day timestamp vector (1440 minutes)\n",
    "        full_day_timestamps = [day_start + pd.Timedelta(minutes=m) for m in range(1440)]\n",
    "        timestamp_vectors[str(day)] = full_day_timestamps\n",
    "        \n",
    "        # Initialize full day meal timing features\n",
    "        meal_timing = np.zeros((5, 1440))  # 5 features for 1440 minutes\n",
    "        \n",
    "        # Set default values for all minutes\n",
    "        meal_timing[0, :] = -1  # Minutes since most recent meal\n",
    "        meal_timing[1, :] = -1  # Minutes until next meal\n",
    "        meal_timing[2, :] = 0   # Is within 2 hours after meal\n",
    "        meal_timing[3, :] = 0   # Count of previous meals\n",
    "        meal_timing[4, :] = np.arange(1440)  # Minutes since start of day\n",
    "        \n",
    "        # Determine time window for processing (we'll still store full day)\n",
    "        if len(meal_times) == 0:\n",
    "            # If no meals, use default window (6 AM to midnight)\n",
    "            window_start = day_start + pd.Timedelta(hours=start_hour)\n",
    "            window_end = day_end\n",
    "        else:\n",
    "            # Start at 6 AM\n",
    "            window_start = day_start + pd.Timedelta(hours=start_hour)\n",
    "            \n",
    "            # End 6 hours after the last meal or at day end, whichever is earlier\n",
    "            last_meal_time = meal_times[-1]\n",
    "            last_meal_plus_6h = last_meal_time + pd.Timedelta(minutes=minutes_after_last_meal)\n",
    "            window_end = min(last_meal_plus_6h, day_end)\n",
    "        \n",
    "        # Filter data to our window\n",
    "        window_data = day_data.loc[window_start:window_end]\n",
    "        \n",
    "        # Calculate window minutes for validation/debugging\n",
    "        window_minutes = len(window_data)\n",
    "        \n",
    "        if not window_data.empty:\n",
    "            # Store CGM and activity data for the window\n",
    "            for j, col in enumerate(cgm_cols):\n",
    "                if col in window_data.columns:\n",
    "                    # Get minutes of day for each window timestamp\n",
    "                    minutes_of_day = (window_data.index.hour * 60 + window_data.index.minute).values\n",
    "                    \n",
    "                    vals = window_data[col].values\n",
    "                    vals = np.nan_to_num(vals, nan=window_data[col].median())\n",
    "                    cgm_daily_data[i, j, minutes_of_day] = vals\n",
    "            \n",
    "            for j, col in enumerate(activity_cols):\n",
    "                if col in window_data.columns:\n",
    "                    # Get minutes of day for each window timestamp\n",
    "                    minutes_of_day = (window_data.index.hour * 60 + window_data.index.minute).values\n",
    "                    \n",
    "                    vals = window_data[col].values\n",
    "                    vals = np.nan_to_num(vals, nan=window_data[col].median())\n",
    "                    activity_daily_data[i, j, minutes_of_day] = vals\n",
    "            \n",
    "            # Calculate meal timing features for each timestamp in the full day\n",
    "            for minute in range(1440):\n",
    "                timestamp = day_start + pd.Timedelta(minutes=minute)\n",
    "                \n",
    "                # 1. Minutes since most recent meal\n",
    "                prev_meals = [m for m in meal_times if m <= timestamp]\n",
    "                if prev_meals:\n",
    "                    meal_timing[0, minute] = (timestamp - prev_meals[-1]).total_seconds()/60\n",
    "                # else keep default -1\n",
    "                \n",
    "                # 2. Minutes until next meal\n",
    "                next_meals = [m for m in meal_times if m > timestamp]\n",
    "                if next_meals:\n",
    "                    meal_timing[1, minute] = (next_meals[0] - timestamp).total_seconds()/60\n",
    "                # else keep default -1\n",
    "                \n",
    "                # 3. Boolean: Is this within 2 hours after a meal?\n",
    "                meal_timing[2, minute] = 1 if (meal_timing[0, minute] >= 0 and meal_timing[0, minute] <= 120) else 0\n",
    "                \n",
    "                # 4. Count of previous meals for the day\n",
    "                meal_timing[3, minute] = len(prev_meals)\n",
    "                \n",
    "                # 5. Minutes since start of day is already set to minute number\n",
    "        \n",
    "        # Store meal timing features (full day)\n",
    "        meal_timing_features[str(day)] = meal_timing\n",
    "        \n",
    "        # Process nutrition data with more detailed information\n",
    "        day_str = str(day)\n",
    "        original_day_data = dataset_df.loc[day_start:day_end]\n",
    "        \n",
    "        nutrition_rows = original_day_data.dropna(subset=['Calories', 'Carbs', 'Protein', 'Fat', 'Fiber'], how='all')\n",
    "        day_nutrition = []\n",
    "        \n",
    "        # Enhanced meal information\n",
    "        meal_counter = {}  # Track meal numbers by type\n",
    "        \n",
    "        # Process meals in chronological order\n",
    "        for ts, row in nutrition_rows.iterrows():\n",
    "            meal_type = row['Meal Type'] if pd.notna(row['Meal Type']) else 'Unknown'\n",
    "            \n",
    "            # Increment meal counter for this type\n",
    "            if meal_type not in meal_counter:\n",
    "                meal_counter[meal_type] = 1\n",
    "            else:\n",
    "                meal_counter[meal_type] += 1\n",
    "                \n",
    "            # Calculate meal timing within day\n",
    "            minutes_since_day_start = (ts - day_start).total_seconds() / 60\n",
    "            hour_of_day = ts.hour + ts.minute/60\n",
    "            \n",
    "            nutrition = {\n",
    "                'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'MealType': meal_type,\n",
    "                'MealNumber': meal_counter[meal_type],  # Which breakfast/lunch/dinner/snack is this?\n",
    "                'MinuteOfDay': int(minutes_since_day_start),\n",
    "                'HourOfDay': hour_of_day,\n",
    "                'calories': row['Calories'] if pd.notna(row['Calories']) else 0,\n",
    "                'carbs': row['Carbs'] if pd.notna(row['Carbs']) else 0,\n",
    "                'protein': row['Protein'] if pd.notna(row['Protein']) else 0,\n",
    "                'fat': row['Fat'] if pd.notna(row['Fat']) else 0,\n",
    "                'fiber': row['Fiber'] if pd.notna(row['Fiber']) else 0,\n",
    "                'has_image': pd.notna(row['Image path'])\n",
    "            }\n",
    "            day_nutrition.append(nutrition)\n",
    "        \n",
    "        # Add meal sequence information\n",
    "        if day_nutrition:\n",
    "            # Sort by timestamp\n",
    "            day_nutrition = sorted(day_nutrition, key=lambda x: x['MinuteOfDay'])\n",
    "            \n",
    "            # Add meal sequence number and intervals\n",
    "            for k in range(len(day_nutrition)):\n",
    "                day_nutrition[k]['MealSequence'] = k + 1  # 1-based meal sequence for the day\n",
    "                \n",
    "                # Time to next meal\n",
    "                if k < len(day_nutrition) - 1:\n",
    "                    day_nutrition[k]['MinutesToNextMeal'] = day_nutrition[k+1]['MinuteOfDay'] - day_nutrition[k]['MinuteOfDay']\n",
    "                else:\n",
    "                    day_nutrition[k]['MinutesToNextMeal'] = -1  # No next meal\n",
    "                \n",
    "                # Time since previous meal\n",
    "                if k > 0:\n",
    "                    day_nutrition[k]['MinutesSincePrevMeal'] = day_nutrition[k]['MinuteOfDay'] - day_nutrition[k-1]['MinuteOfDay']\n",
    "                else:\n",
    "                    day_nutrition[k]['MinutesSincePrevMeal'] = -1  # No previous meal\n",
    "        \n",
    "        nutrition_data_by_day[day_str] = day_nutrition\n",
    "        \n",
    "        # Image data processing remains largely the same\n",
    "        image_rows = original_day_data.dropna(subset=['Image path'])\n",
    "        day_images = []\n",
    "        for ts, row in image_rows.iterrows():\n",
    "            try:\n",
    "                img_data = get_image(row['Image path'], subject_id, img_size)\n",
    "                # Calculate timing features for this image/meal\n",
    "                minutes_since_day_start = (ts - day_start).total_seconds() / 60\n",
    "                \n",
    "                metadata = {\n",
    "                    'timestamp': ts.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'minute_of_day': int(minutes_since_day_start),\n",
    "                    'meal_type': row['Meal Type'] if 'Meal Type' in row else None,\n",
    "                    'calories': row['Calories'] if 'Calories' in row else None,\n",
    "                    'carbs': row['Carbs'] if 'Carbs' in row else None,\n",
    "                    'protein': row['Protein'] if 'Protein' in row else None,\n",
    "                    'fat': row['Fat'] if 'Fat' in row else None,\n",
    "                    'fiber': row['Fiber'] if 'Fiber' in row else None\n",
    "                }\n",
    "                day_images.append({'image': img_data, 'metadata': metadata})\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "        image_data_by_day[day_str] = day_images if day_images else []\n",
    "\n",
    "    # Window metadata\n",
    "    window_metadata = {\n",
    "        'start_hour': start_hour,\n",
    "        'hours_after_last_meal': minutes_after_last_meal / 60,\n",
    "        'full_day_length': 1440\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        days_list,\n",
    "        cgm_daily_data,\n",
    "        activity_daily_data,\n",
    "        image_data_by_day,\n",
    "        nutrition_data_by_day,\n",
    "        cgm_stats,\n",
    "        activity_stats,\n",
    "        window_metadata,\n",
    "        timestamp_vectors,        # Now: full day timestamps\n",
    "        meal_timing_features      # Now: full day meal timing features\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CGMacros(\n",
    "    subject_id: int,\n",
    "    csv_dir: str = \"CGMacros-2\",\n",
    ") -> pd.DataFrame:\n",
    "    if type(subject_id) != int:\n",
    "        print(\"subject_id should be an integer\")\n",
    "        raise ValueError\n",
    "    subejct_path = f\"CGMacros-{subject_id:03d}/CGMacros-{subject_id:03d}.csv\"\n",
    "    subject_file = os.path.join(csv_dir, subejct_path)\n",
    "    if not os.path.exists(subject_file):\n",
    "        tqdm.write(f\"File {subject_file} not found\")\n",
    "        raise FileNotFoundError\n",
    "    dataset_df = pd.read_csv(subject_file, index_col=None)\n",
    "    dataset_df[\"Timestamp\"] = pd.to_datetime(dataset_df[\"Timestamp\"])\n",
    "    dataset_df = clean_time_series_data(dataset_df)  # Add cleaning step\n",
    "    return dataset_df.set_index(\"Timestamp\")\n",
    "\n",
    "def get_image(\n",
    "    img_filename: str,\n",
    "    subject_id: int,\n",
    "    target_size: tuple,\n",
    "    cgmacros_path: str = \"CGMacros-2/\",\n",
    ") -> np.ndarray:\n",
    "    subject_path = f\"CGMacros-{subject_id:03d}/\"\n",
    "    img_path = f\"{cgmacros_path}{subject_path}{img_filename}\"\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"File {img_path} not found\")\n",
    "        raise FileNotFoundError\n",
    "    # Loading names out\n",
    "    img_data = cv2.resize(\n",
    "        cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB),\n",
    "        target_size,\n",
    "        interpolation=cv2.INTER_LANCZOS4,\n",
    "    )\n",
    "    return img_data\n",
    "def create_daily_dataset(\n",
    "    subject_id: int,\n",
    "    csv_dir: str = \"CGMacros 2\",\n",
    "    cgm_cols=[\"Dexcom GL\", \"Libre GL\"],\n",
    "    activity_cols=[\"HR\",\"METs\"],\n",
    "    img_size=(112, 112),\n",
    "    start_hour=6,\n",
    "    verbose=False\n",
    "):\n",
    "    try:\n",
    "        # Load data with column validation\n",
    "        dataset_df = load_CGMacros(subject_id, csv_dir)\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Available columns:\", dataset_df.columns.tolist())\n",
    "        \n",
    "        # Handle missing columns gracefully\n",
    "        available_activity_cols = [col for col in activity_cols \n",
    "                                 if col in dataset_df.columns]\n",
    "        if len(available_activity_cols) < len(activity_cols):\n",
    "            print(f\"Warning: Missing activity columns. Using {available_activity_cols} for subject {subject_id}\")\n",
    "        \n",
    "        # Process data with validated columns and custom time window\n",
    "        result = load_daily_traces(\n",
    "            dataset_df, subject_id, \n",
    "            cgm_cols=cgm_cols,\n",
    "            activity_cols=available_activity_cols,\n",
    "            img_size=img_size,\n",
    "            start_hour=start_hour\n",
    "        )\n",
    "        \n",
    "        return (subject_id,) + result  # Return all elements with subject_id\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Data for subject {subject_id} not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject {subject_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_subjects(\n",
    "    subject_ids=None,\n",
    "    csv_dir=\"CGMacros-2\",\n",
    "    demographics_path = \"demographicPCA.csv\",\n",
    "    save_dir=\"processed_data/\",\n",
    "    cgm_cols=[\"Dexcom GL\",\"Libre GL\"],\n",
    "    activity_cols=[\"HR\",\"METs\"],\n",
    "    img_size=(112, 112),\n",
    "    start_hour=6\n",
    "):\n",
    "    # Create save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    if subject_ids is None:\n",
    "        subject_ids = range(1, 51)  # Try subjects 1-50\n",
    "    \n",
    "    summary = {\n",
    "        'processed_subjects': [],\n",
    "        'total_days': 0,\n",
    "        'total_images': 0,\n",
    "        'total_meals': 0\n",
    "    }\n",
    "    \n",
    "    for subject_id in tqdm(subject_ids, desc=\"Processing subjects\"):\n",
    "        result = create_daily_dataset(subject_id, csv_dir, start_hour=start_hour)\n",
    "        if result is None:\n",
    "            continue\n",
    "            \n",
    "        # Unpack all return values\n",
    "        (subject_id, days, cgm, activity, images, nutrition, \n",
    "         cgm_stats, activity_stats, window_metadata, \n",
    "         timestamp_vectors, meal_timing_features) = result\n",
    "        \n",
    "        pca_df = pd.read_csv(demographics_path)\n",
    "        pca_vector = pca_df[pca_df['SubjectID'] == subject_id].iloc[0, 1:].values.astype(\"float32\")\n",
    "\n",
    "        \n",
    "        # Save data with all new features\n",
    "        subject_data = {\n",
    "            'subject_id': subject_id,\n",
    "            'days': days,\n",
    "            'cgm_data': cgm,\n",
    "            'activity_data': activity,\n",
    "            'image_data': images,\n",
    "            'nutrition_data': nutrition,\n",
    "            'cgm_stats': cgm_stats,\n",
    "            'activity_stats': activity_stats,\n",
    "            'demographics':pca_vector,\n",
    "            'window_metadata': window_metadata,\n",
    "            'timestamp_vectors': timestamp_vectors,  # New: store timestamps\n",
    "            'meal_timing_features': meal_timing_features  # New: meal timing features\n",
    "        }\n",
    "        torch.save(subject_data, os.path.join(save_dir, f\"subject_{subject_id:03d}_daily_data.pt\"))\n",
    "        \n",
    "        # Update summary counts\n",
    "        summary['processed_subjects'].append(subject_id)\n",
    "        summary['total_days'] += len(days)\n",
    "        summary['total_images'] += sum(len(imgs) for imgs in images.values())\n",
    "        summary['total_meals'] += sum(len(meals) for meals in nutrition.values())\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyTracesDataset(Dataset):\n",
    "    def __init__(self, data_dir, subject_ids=None, transform=None, skip_days=[1]):\n",
    "        # Initialization remains the same\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.skip_days = skip_days or [1]\n",
    "        transform = None\n",
    "        \n",
    "        # Find relevant subject files\n",
    "        if subject_ids is None:\n",
    "            self.data_files = [\n",
    "                f for f in os.listdir(data_dir) \n",
    "                if f.startswith(\"subject_\") and f.endswith(\"_daily_data.pt\")\n",
    "            ]\n",
    "        else:\n",
    "            self.data_files = [\n",
    "                f\"subject_{sid:03d}_daily_data.pt\" \n",
    "                for sid in subject_ids \n",
    "                if os.path.exists(os.path.join(data_dir, f\"subject_{sid:03d}_daily_data.pt\"))\n",
    "            ]\n",
    "        \n",
    "        # Build indices accounting for skip_days\n",
    "        self.indices = []\n",
    "        self.subject_day_pairs = []\n",
    "        \n",
    "        for file_idx, fname in enumerate(self.data_files):\n",
    "            data = torch.load(os.path.join(data_dir, fname), weights_only=False)\n",
    "            subject_id = data['subject_id']\n",
    "            \n",
    "            for day_idx, day_str in enumerate(data['days']):\n",
    "                day_num = int(day_str.split('-')[2])\n",
    "                if day_num not in self.skip_days:\n",
    "                    self.indices.append((file_idx, day_idx))\n",
    "                    self.subject_day_pairs.append((subject_id, day_num))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, day_idx = self.indices[idx]\n",
    "        data = torch.load(os.path.join(self.data_dir, self.data_files[file_idx]), weights_only=False)\n",
    "        day = data['days'][day_idx]\n",
    "        \n",
    "        # Apply transforms if specified\n",
    "        def _apply_transform(x):\n",
    "            return self.transform(x) if self.transform else x\n",
    "        \n",
    "        demographics_path = 'demographicPCA.csv'\n",
    "        pca_df = pd.read_csv(demographics_path)\n",
    "        subject_data = pca_df[pca_df['SubjectID'] == data['subject_id']]\n",
    "        pca_values = subject_data[['PCA_1', 'PCA_2', 'PCA_3', 'PCA_4', 'PCA_5']].values\n",
    "\n",
    "        # Include the new features in the returned item\n",
    "        return {\n",
    "            'subject_id': data['subject_id'],\n",
    "            'day': day,\n",
    "            'cgm_data': _apply_transform(data['cgm_data'][day_idx]),\n",
    "           'demographics':  torch.tensor(pca_values.flatten(), dtype=torch.float32),\n",
    "            'activity_data': _apply_transform(data['activity_data'][day_idx]),\n",
    "            'images': [_apply_transform(img['image']) for img in data['image_data'].get(day, [])],\n",
    "            'nutrition': data['nutrition_data'].get(day, []),\n",
    "            'subject_day_pair': self.subject_day_pairs[idx],\n",
    "            'timestamps': data.get('timestamp_vectors', {}).get(day, []),  # New: timestamps\n",
    "            'meal_timing_features': data.get('meal_timing_features', {}).get(day, [])  # New: meal timing features\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    \"\"\"Handles variable-length nutrition data, images and timestamp vectors\"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def fix_nans(array):\n",
    "        \"\"\"Replace NaNs with median (per channel)\"\"\"\n",
    "        median_vals = np.nanmedian(array, axis=1, keepdims=True)\n",
    "        return np.where(np.isnan(array), median_vals, array)\n",
    "\n",
    "    # Fix NaNs before converting to tensors\n",
    "    for i, x in enumerate(batch):\n",
    "        x['cgm_data'] = fix_nans(x['cgm_data'])\n",
    "        x['activity_data'] = fix_nans(x['activity_data'])\n",
    "        if 'meal_timing_features' in x and len(x['meal_timing_features']) > 0:\n",
    "            x['meal_timing_features'] = fix_nans(x['meal_timing_features'])\n",
    "\n",
    "    return {\n",
    "        'subject_ids': torch.tensor([x['subject_id'] for x in batch]),\n",
    "        'days': [x['day'] for x in batch],\n",
    "        \n",
    "        'cgm_data': torch.stack([torch.tensor(x['cgm_data'], dtype=torch.float32) for x in batch]),\n",
    "        'activity_data': torch.stack([torch.tensor(x['activity_data'], dtype=torch.float32) for x in batch]),\n",
    "        'images': [x['images'] for x in batch],  # List of lists\n",
    "        'nutrition': [x['nutrition'] for x in batch],  # List of lists\n",
    "        'subject_day_pairs': [x['subject_day_pair'] for x in batch],\n",
    "        'timestamps': [x.get('timestamps', []) for x in batch],  # New: timestamps for each data point\n",
    "        'meal_timing_features': [torch.tensor(x.get('meal_timing_features', np.zeros((5, 1))), \n",
    "                                              dtype=torch.float32) if len(x.get('meal_timing_features', [])) > 0 \n",
    "                                 else torch.zeros((5, 1)) for x in batch],  # New: meal timing features,\n",
    "        'demographics': torch.stack([x['demographics'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_by_subject_day(dataset, test_size=0.2, random_state=2025):\n",
    "    \"\"\"\n",
    "    Split the dataset based on subject-day pairs to ensure all data from\n",
    "    the same subject and day stays together in either training or testing set.\n",
    "    \n",
    "    Args:\n",
    "        dataset (DailyTracesDataset): The dataset to split\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        random_state (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_indices, test_indices)\n",
    "    \"\"\"\n",
    "    # Get unique subject-day pairs\n",
    "    subject_day_df = pd.DataFrame(dataset.subject_day_pairs, columns=['subject_id', 'day_id'])\n",
    "    unique_pairs = subject_day_df.drop_duplicates()\n",
    "    \n",
    "    # Split the unique subject-day pairs\n",
    "    train_pairs, test_pairs = train_test_split(\n",
    "        unique_pairs, \n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Convert to sets for faster lookup\n",
    "    train_pairs_set = set(zip(train_pairs['subject_id'], train_pairs['day_id']))\n",
    "    test_pairs_set = set(zip(test_pairs['subject_id'], test_pairs['day_id']))\n",
    "    \n",
    "    # Create masks for train and test indices\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for i, (subject_id, day_id) in enumerate(dataset.subject_day_pairs):\n",
    "        if (subject_id, day_id) in train_pairs_set:\n",
    "            train_indices.append(i)\n",
    "        elif (subject_id, day_id) in test_pairs_set:\n",
    "            test_indices.append(i)\n",
    "    \n",
    "    return train_indices, test_indices\n",
    "\n",
    "\n",
    "class SubjectDaySubset(Dataset):\n",
    "    \"\"\"\n",
    "    Subset of DailyTracesDataset based on indices.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "def get_train_test_datasets(data_dir, subject_ids=None, test_size=0.2, random_state=2025, transform=None):\n",
    "    \"\"\"\n",
    "    Get train and test datasets split by subject-day pairs.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing processed data\n",
    "        subject_ids (list): List of subject IDs to include. If None, include all available.\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        random_state (int): Random seed for reproducibility\n",
    "        transform (callable): Optional transform to apply to the data\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_dataset, test_dataset)\n",
    "    \"\"\"\n",
    "    # Create the full dataset\n",
    "    full_dataset = DailyTracesDataset(data_dir, subject_ids, transform)\n",
    "    \n",
    "    # Split by subject-day pairs\n",
    "    train_indices, test_indices = split_dataset_by_subject_day(full_dataset, test_size, random_state)\n",
    "    \n",
    "    # Create train and test subsets\n",
    "    train_dataset = SubjectDaySubset(full_dataset, train_indices)\n",
    "    test_dataset = SubjectDaySubset(full_dataset, test_indices)\n",
    "    \n",
    "    print(f\"Full dataset size: {len(full_dataset)}\")\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples_by_nutrition(dataset):\n",
    "    \"\"\"\n",
    "    Filters samples that don't meet nutrition quality requirements:\n",
    "    - At least 2 distinct meals out of breakfast, lunch, dinner\n",
    "    - Non-zero total calories\n",
    "    - Total calories >= 300\n",
    "    \"\"\"\n",
    "    valid_meals = {\"breakfast\", \"lunch\", \"dinner\"}\n",
    "    filtered = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        nutrition = sample.get(\"nutrition\", [])\n",
    "        if not nutrition or not isinstance(nutrition, list):\n",
    "            continue\n",
    "\n",
    "        meal_types_present = {meal.get(\"MealType\", \"\").lower() for meal in nutrition}\n",
    "        relevant_meals = meal_types_present & valid_meals\n",
    "\n",
    "        total_calories = sum(meal.get(\"calories\", 0) for meal in nutrition)\n",
    "        calories_are_valid = total_calories > 0 and total_calories >= 300\n",
    "\n",
    "        if len(relevant_meals) >= 2 and calories_are_valid:\n",
    "            filtered.append(sample)\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def main():\n",
    "    # Step 1: Process the raw CSV data for subjects and save the daily traces\n",
    "    # You can adjust the subject IDs, directories, etc. as needed\n",
    "    summary = process_multiple_subjects(\n",
    "        subject_ids=range(1, 50),  # Process subjects 1-49\n",
    "        csv_dir=\"CGMacros-2\",  # Path to your CSV files\n",
    "        save_dir=\"processed_data/\",  # Where to save processed data\n",
    "        cgm_cols=[\"Dexcom GL\", \"Libre GL\"],\n",
    "        activity_cols=[\"HR\",\"METs\"],\n",
    "        img_size=(112, 112),\n",
    "        start_hour=6  # New parameter: starting hour (6 AM)\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing summary:\")\n",
    "    print(f\"- Processed {len(summary['processed_subjects'])} subjects\")\n",
    "    print(f\"- Total days: {summary['total_days']}\")\n",
    "    print(f\"- Total images: {summary['total_images']}\")\n",
    "    print(f\"- Total meals: {summary['total_meals']}\")  # Added meal count\n",
    "    \n",
    "    # Step 2: Create train and test datasets from the processed data\n",
    "    train_dataset, test_dataset = get_train_test_datasets(\n",
    "        data_dir=\"processed_data/\",\n",
    "        subject_ids=None,  # Use all available subjects\n",
    "        test_size=0.2,  # 80% train, 20% test\n",
    "        random_state=2025,  # For reproducibility\n",
    "        transform=None  # Add any transforms you need\n",
    "    )\n",
    "    \n",
    "    print(\"Done creating train and test datasets\")\n",
    "    \n",
    "    \n",
    "\n",
    "    #Step 3: Create DataLoaders for efficient batching\n",
    "    train_loader = DataLoader(\n",
    "        filter_samples_by_nutrition(train_dataset),\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        filter_samples_by_nutrition(test_dataset),\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=custom_collate\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:   0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros-2/CGMacros-001/photos/00000075-PHOTO-2019-4-5-9-48-0.jpg not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:   2%|▏         | 1/49 [00:03<02:58,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros-2/CGMacros-002/photos/00000082-PHOTO-2021-1-12-11-37-0.jpg not found\n",
      "File CGMacros-2/CGMacros-002/photos/00000083-PHOTO-2021-1-12-12-13-0.jpg not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  47%|████▋     | 23/49 [01:02<01:17,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros-2/CGMacros-024/CGMacros-024.csv not found\n",
      "Data for subject 24 not found.\n",
      "File CGMacros-2/CGMacros-025/CGMacros-025.csv not found\n",
      "Data for subject 25 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  73%|███████▎  | 36/49 [01:27<00:30,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros-2/CGMacros-037/CGMacros-037.csv not found\n",
      "Data for subject 37 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects:  80%|███████▉  | 39/49 [01:32<00:19,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File CGMacros-2/CGMacros-040/CGMacros-040.csv not found\n",
      "Data for subject 40 not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects: 100%|██████████| 49/49 [01:53<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing subject 49: cannot reindex on an axis with duplicate labels\n",
      "Processing summary:\n",
      "- Processed 44 subjects\n",
      "- Total days: 521\n",
      "- Total images: 3172\n",
      "- Total meals: 1689\n",
      "Full dataset size: 499\n",
      "Train dataset size: 399\n",
      "Test dataset size: 100\n",
      "Done creating train and test datasets\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_calorie_stats(data_loader):\n",
    "    \"\"\"\n",
    "    Computes the mean and std of daily total calories across the dataset.\n",
    "    \n",
    "    Each batch is a dict with 'nutrition' key, which contains a list of days,\n",
    "    where each day is a list of meal dicts.\n",
    "    \"\"\"\n",
    "    total_calories_per_day = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        for day_meals in batch[\"nutrition\"]:  # each element is a day's meals\n",
    "            if isinstance(day_meals, list):\n",
    "                day_total = sum(meal.get(\"calories\", 0) for meal in day_meals if isinstance(meal, dict))\n",
    "                if day_total > 800:\n",
    "                    total_calories_per_day.append(day_total)\n",
    "\n",
    "    calories_tensor = torch.tensor(total_calories_per_day, dtype=torch.float32)\n",
    "    return calories_tensor.mean().item(), calories_tensor.std().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_calorie_stats_lunch(data_loader):\n",
    "    \"\"\"\n",
    "    Computes the mean and std of daily total calories and lunch calories across the dataset.\n",
    "    \n",
    "    Each batch is a dict with 'nutrition' key, which contains a list of days,\n",
    "    where each day is a list of meal dicts.\n",
    "    \n",
    "    Returns:\n",
    "        (total_mean, total_std), (lunch_mean, lunch_std)\n",
    "    \"\"\"\n",
    "    total_calories_per_day = []\n",
    "    lunch_calories_per_day = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        for day_meals in batch[\"nutrition\"]:  # each element is a day's meals\n",
    "            if isinstance(day_meals, list):\n",
    "                # Total calories\n",
    "                day_total = sum(meal.get(\"calories\", 0) for meal in day_meals if isinstance(meal, dict))\n",
    "                if day_total > 800:\n",
    "                    total_calories_per_day.append(day_total)\n",
    "\n",
    "                # Lunch-only calories\n",
    "                lunch_total = sum(\n",
    "                    meal.get(\"calories\", 0)\n",
    "                    for meal in day_meals\n",
    "                    if isinstance(meal, dict) and meal.get(\"MealType\") == \"lunch\"\n",
    "                )\n",
    "                if lunch_total > 0:\n",
    "                    lunch_calories_per_day.append(lunch_total)\n",
    "\n",
    "    total_tensor = torch.tensor(total_calories_per_day, dtype=torch.float32)\n",
    "    lunch_tensor = torch.tensor(lunch_calories_per_day, dtype=torch.float32)\n",
    "\n",
    "    return (total_tensor.mean().item(), total_tensor.std().item()), (\n",
    "        lunch_tensor.mean().item(), lunch_tensor.std().item()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, lunchTuple =  compute_calorie_stats_lunch(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573.459228515625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_mean, global_std = compute_calorie_stats(train_loader)\n",
    "global_mean, global_std = float(global_mean), float(global_std)\n",
    "global_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1911.24658203125"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labels(labels,mean,std, device, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Processes labels by summing the 'calories' field for each instance in a batch.\n",
    "\n",
    "    Args:\n",
    "        labels (list of list of dicts): A batch of labels, where each instance is a list of meal records.\n",
    "        device (str): Target device (\"cuda:0\" or \"cpu\").\n",
    "        dtype (torch.dtype): Data type (default: float16 for mixed precision).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (batch_size,) with summed calories per instance.\n",
    "    \"\"\"\n",
    "    batch_calories = [\n",
    "        sum(entry.get(\"calories\", 0) for entry in instance)  \n",
    "        for instance in labels\n",
    "    ]\n",
    "    labels_tensor = torch.tensor(batch_calories, dtype=dtype).to(device)\n",
    "    normalized_labels = (labels_tensor - mean) / (std + 1e-8)\n",
    "    \n",
    "    return normalized_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # Import torch.nn\n",
    "import torch.nn.functional as F \n",
    "class RMSRELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon  # Small value to avoid division by zero\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        relative_error = (pred - target) / (target + self.epsilon)\n",
    "        squared_rel_error = relative_error ** 2\n",
    "        mean_squared_rel_error = torch.mean(squared_rel_error)\n",
    "        return torch.sqrt(mean_squared_rel_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['subject_ids', 'days', 'cgm_data', 'activity_data', 'images', 'nutrition', 'subject_day_pairs', 'timestamps', 'meal_timing_features', 'demographics'])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1440])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['meal_timing_features'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(\n",
    "#     model, \n",
    "#     train_loader, \n",
    "#     val_loader,\n",
    "#     global_mean,\n",
    "#     global_std,\n",
    "#     device_activity=\"cuda:0\", \n",
    "#     device_cgm=\"cuda:1\", \n",
    "#     device_meal=\"cuda:0\",  \n",
    "#     device_regressor=\"cuda:0\",\n",
    "#     epochs=30, \n",
    "#     lr=5e-4\n",
    "# ):\n",
    "#     # Move model to respective GPUs\n",
    "#     model.to(device_regressor)\n",
    "    \n",
    "#     # Loss function and optimizer\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "#     # Store loss values\n",
    "#     training_losses = []\n",
    "#     validation_losses = []\n",
    "    \n",
    "#     # Training loop\n",
    "#     for epoch in tqdm(range(epochs), ascii=True, desc=\"Training Epochs\"):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0.0\n",
    "#         for batch in train_loader:\n",
    "#             # Unpack data\n",
    "#             activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "#             cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "#             meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "#             demographics = batch['demographics'].to(device_regressor)  # Demographics tensor\n",
    "\n",
    "\n",
    "#             labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "#             labels = labels.float().to(device_regressor)\n",
    "            \n",
    "#             # Zero gradients\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             # Forward pass\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 pred = model(cgm_data, activity_data, meal_timing_data, demographics)\n",
    "                \n",
    "#                 # Compute loss\n",
    "#                 loss = criterion(pred, labels).mean(dim=0)\n",
    "            \n",
    "#             # Backpropagation\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "        \n",
    "#         avg_train_loss = epoch_loss / len(train_loader)\n",
    "#         training_losses.append(avg_train_loss)\n",
    "#         print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "#         # Validation Loop\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 # Unpack data\n",
    "#                 activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "#                 cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "#                 meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "#                 demographics = batch['demographics'].to(device_regressor)\n",
    "#                 labels = process_labels(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "#                 labels = labels.float().to(device_regressor)\n",
    "                \n",
    "#                 # Forward pass\n",
    "#                 pred = model(cgm_data, activity_data, meal_timing_data, demographics)\n",
    "                \n",
    "#                 # Compute loss\n",
    "#                 loss = criterion(pred, labels).mean(dim=0)\n",
    "#                 val_loss += loss.item()\n",
    "            \n",
    "#             avg_val_loss = val_loss / len(val_loader)\n",
    "#             validation_losses.append(avg_val_loss)\n",
    "#             print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "#     print(\"Training Complete!\")\n",
    "#     return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labels_lunch_only(labels, mean, std, device, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Processes labels by summing the 'calories' of 'lunch' meals for each instance in a batch.\n",
    "\n",
    "    Args:\n",
    "        labels (list of list of dicts): A batch of labels, where each instance is a list of meal records.\n",
    "        mean (float): Mean used for normalization.\n",
    "        std (float): Standard deviation used for normalization.\n",
    "        device (str): Target device (\"cuda:0\" or \"cpu\").\n",
    "        dtype (torch.dtype): Data type (default: float32).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape (batch_size,) with summed lunch calories per instance.\n",
    "    \"\"\"\n",
    "    batch_calories = [\n",
    "        sum(entry.get(\"calories\", 0) for entry in instance if entry.get(\"MealType\") == \"lunch\")  \n",
    "        for instance in labels\n",
    "    ]\n",
    "    labels_tensor = torch.tensor(batch_calories, dtype=dtype).to(device)\n",
    "    normalized_labels = (labels_tensor - mean) / (std + 1e-8)\n",
    "\n",
    "    return normalized_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(\n",
    "    activity_encoder, cgm_encoder, meal_time_encoder, regressor, \n",
    "    train_loader, val_loader, global_mean, global_std,\n",
    "    device_activity=\"cuda:0\", device_cgm=\"cuda:1\", device_meal=\"cuda:0\", device_regressor=\"cuda:0\",\n",
    "    epochs=30, lr=5e-4, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with multiple encoders and demographics integration at regressor level.\n",
    "    \n",
    "    Args:\n",
    "        activity_encoder: Encoder for activity data\n",
    "        cgm_encoder: Encoder for CGM data\n",
    "        meal_time_encoder: Encoder for meal timing features\n",
    "        regressor: Final regressor that combines all features\n",
    "        train_loader, val_loader: Data loaders\n",
    "        global_mean, global_std: Normalization parameters\n",
    "        device_*: Device assignments for different components\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        device: Default device if specific devices not available\n",
    "    \"\"\"\n",
    "    # Check device availability and set device configuration\n",
    "    if not torch.cuda.is_available() and \"cuda\" in (device_activity, device_cgm, device_meal, device_regressor):\n",
    "        print(\"CUDA not available. Falling back to CPU.\")\n",
    "        device_activity = device_cgm = device_meal = device_regressor = \"cpu\"\n",
    "    elif torch.cuda.device_count() == 1 and any(d != \"cuda:0\" for d in [device_activity, device_cgm, device_meal, device_regressor]):\n",
    "        print(f\"Only one CUDA device available. Using cuda:0 for all components.\")\n",
    "        device_activity = device_cgm = device_meal = device_regressor = \"cuda:0\"\n",
    "    \n",
    "    # Move models to respective devices\n",
    "    activity_encoder.to(device_activity)\n",
    "    cgm_encoder.to(device_cgm)\n",
    "    meal_time_encoder.to(device_meal)\n",
    "    regressor.to(device_regressor)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = RMSRELoss()\n",
    "    \n",
    "    # Create a single optimizer for all parameters\n",
    "    optimizer = optim.Adam(\n",
    "        list(activity_encoder.parameters()) +\n",
    "        list(cgm_encoder.parameters()) +\n",
    "        list(meal_time_encoder.parameters()) +\n",
    "        list(regressor.parameters()), \n",
    "        lr=lr\n",
    "    )\n",
    "    \n",
    "    # Store loss values\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # Determine if we can use mixed precision\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), ascii=True, desc=\"Training Epochs\"):\n",
    "        # Set models to training mode\n",
    "        activity_encoder.train()\n",
    "        cgm_encoder.train()\n",
    "        meal_time_encoder.train()\n",
    "        regressor.train()\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            # Extract and move data to respective devices\n",
    "            activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "            cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "            meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "            demographics = batch['demographics'].to(device_regressor)\n",
    "            \n",
    "            # Process labels\n",
    "            labels = process_labels_lunch_only(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "            labels = labels.float().to(device_regressor)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with optional mixed precision\n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    # Generate embeddings from each encoder\n",
    "                    activity_emb = activity_encoder(activity_data)\n",
    "                    cgm_emb = cgm_encoder(cgm_data)\n",
    "                    meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                    \n",
    "                    # Move embeddings to regressor device\n",
    "                    activity_emb = activity_emb.to(device_regressor)\n",
    "                    cgm_emb = cgm_emb.to(device_regressor)\n",
    "                    meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                    \n",
    "                    # Concatenate embeddings with demographics\n",
    "                    joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, demographics], dim=1).to(torch.float32)\n",
    "                    \n",
    "                    # Final prediction\n",
    "                    pred = regressor(joint_emb).squeeze(1)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = criterion(pred, labels).mean(dim=0)\n",
    "                \n",
    "                # Backpropagation with scaler\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard processing without mixed precision\n",
    "                activity_emb = activity_encoder(activity_data)\n",
    "                cgm_emb = cgm_encoder(cgm_data)\n",
    "                meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                \n",
    "                # Move embeddings to regressor device\n",
    "                activity_emb = activity_emb.to(device_regressor)\n",
    "                cgm_emb = cgm_emb.to(device_regressor)\n",
    "                meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                \n",
    "                # Concatenate embeddings with demographics\n",
    "                joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, demographics], dim=1).to(torch.float32)\n",
    "                \n",
    "                # Final prediction\n",
    "                pred = regressor(joint_emb)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels).mean(dim=0)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        training_losses.append(avg_train_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation Loop\n",
    "        activity_encoder.eval()\n",
    "        cgm_encoder.eval()\n",
    "        meal_time_encoder.eval()\n",
    "        regressor.eval()\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Extract and move data to respective devices\n",
    "                activity_data = batch[\"activity_data\"].to(device_activity)\n",
    "                cgm_data = batch['cgm_data'][:, 0, :].unsqueeze(1).to(device_cgm)\n",
    "                meal_timing_data = torch.stack(batch['meal_timing_features'], dim=0).to(device_meal)\n",
    "                demographics = batch['demographics'].to(device_regressor)\n",
    "                \n",
    "                # Process labels\n",
    "                labels = process_labels_lunch_only(batch[\"nutrition\"], global_mean, global_std, device_regressor)\n",
    "                labels = labels.float().to(device_regressor)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                activity_emb = activity_encoder(activity_data)\n",
    "                cgm_emb = cgm_encoder(cgm_data)\n",
    "                meal_time_emb = meal_time_encoder(meal_timing_data)\n",
    "                \n",
    "                # Move embeddings to regressor device\n",
    "                activity_emb = activity_emb.to(device_regressor)\n",
    "                cgm_emb = cgm_emb.to(device_regressor)\n",
    "                meal_time_emb = meal_time_emb.to(device_regressor)\n",
    "                \n",
    "                # Concatenate embeddings with demographics\n",
    "                joint_emb = torch.cat([cgm_emb, activity_emb, meal_time_emb, demographics], dim=1).to(torch.float32)\n",
    "                \n",
    "                # Final prediction\n",
    "                pred = regressor(joint_emb)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(pred, labels).mean(dim=0)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            validation_losses.append(avg_val_loss)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training Complete!\")\n",
    "    return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import MultiheadAttention as TransformerEncoder\n",
    "import torch.nn as nn\n",
    "class CaloricRegressor(nn.Module):\n",
    "    def __init__(self, cgm_emb_size, activity_emb_size, meal_timing_emb_size, \n",
    "                 demographics_size=5, hidden_size=128, output_size=1):\n",
    "        super(CaloricRegressor, self).__init__()\n",
    "        \n",
    "        total_input_size = cgm_emb_size + activity_emb_size + meal_timing_emb_size + demographics_size\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(total_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.regressor(x)\n",
    "    \n",
    "class MealTimingEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=5, hidden_size=64, output_size=32):\n",
    "        super(MealTimingEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, hidden_size, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(hidden_size, hidden_size*2, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv1d(hidden_size*2, hidden_size*4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size*4, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channels=5, seq_len=1440]\n",
    "        x = self.encoder(x)\n",
    "        x = x.squeeze(-1)  # Remove the last dimension after global pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class CaloricModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CaloricModel, self).__init__()\n",
    "        self.cgm_encoder = TransformerEncoder(n_features=1440, embed_dim=96, num_heads=2, num_classes=64, dropout=0.2, num_layers=6)\n",
    "        self.activity_encoder = TransformerEncoder(n_features=1440, embed_dim=96, num_heads=2, num_classes=64, dropout=0.2, num_layers=6)\n",
    "        self.meal_timing_encoder = MealTimingEncoder(input_channels=5, hidden_size=64, output_size=32)\n",
    "        self.caloric_regressor = CaloricRegressor(\n",
    "            cgm_emb_size=64,\n",
    "            activity_emb_size=64,\n",
    "            meal_timing_emb_size=32,\n",
    "            demographics_size=5\n",
    "        )\n",
    "\n",
    "    def forward(self, cgm, activity, meal_timing, demographics):\n",
    "        cgm_embed = self.cgm_encoder(cgm)\n",
    "        activity_embed = self.activity_encoder(activity)\n",
    "        meal_embed = self.meal_timing_encoder(meal_timing)\n",
    "\n",
    "        # Concatenate all embeddings and demographic vector\n",
    "        x = torch.cat([cgm_embed, activity_embed, meal_embed, demographics], dim=1)\n",
    "        return self.caloric_regressor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_550528/998161765.py:59: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
      "Training Epochs:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformer import MultiheadAttention as TransformerEncoder\n",
    "from multiChannelTransformer import MultiChannelTransformerEncoder \n",
    "from joint_layers import Regressor\n",
    "activity_encoder = MultiChannelTransformerEncoder(\n",
    "    n_features=1440,  # Your sequence length\n",
    "    n_channels=2,     # Number of channels in your tensor\n",
    "    embed_dim=96,\n",
    "    num_heads=2,\n",
    "    num_classes=64,\n",
    "    dropout=0.2,\n",
    "    num_layers=3\n",
    ")\n",
    "cgm_encoder = TransformerEncoder(n_features=1440, embed_dim=96, num_heads=2, num_classes=64, dropout=0.2, num_layers=3)\n",
    "meal_time_encoder = meal_time_encoder = MultiChannelTransformerEncoder(\n",
    "    n_features=1440,  # Your sequence length\n",
    "    n_channels=5,     # Number of channels in your tensor\n",
    "    embed_dim=96,\n",
    "    num_heads=2,\n",
    "    num_classes=64,\n",
    "    dropout=0.2,\n",
    "    num_layers=3\n",
    ")\n",
    "# Initialize regressor\n",
    "regressor = Regressor(\n",
    "    input_size=64+64+64+5,  # Sum of all embedding dimensions plus demographics\n",
    "    hidden=128,\n",
    "    output_size=1,  # Adjust based on your nutrition prediction targets\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# Create the final model\n",
    "training_losses, validation_losses = train_model(activity_encoder,cgm_encoder,meal_time_encoder,regressor,train_loader,test_loader,lunchTuple[0],lunchTuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1440])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['activity_data'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'timestamp': '2019-04-07 07:29:00',\n",
       "   'MealType': 'breakfast',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 449,\n",
       "   'HourOfDay': 7.483333333333333,\n",
       "   'calories': 712.0,\n",
       "   'carbs': 66.0,\n",
       "   'protein': 22.0,\n",
       "   'fat': 42.0,\n",
       "   'fiber': 0.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 1,\n",
       "   'MinutesToNextMeal': 294,\n",
       "   'MinutesSincePrevMeal': -1},\n",
       "  {'timestamp': '2019-04-07 12:23:00',\n",
       "   'MealType': 'lunch',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 743,\n",
       "   'HourOfDay': 12.383333333333333,\n",
       "   'calories': 445.0,\n",
       "   'carbs': 43.0,\n",
       "   'protein': 20.0,\n",
       "   'fat': 20.0,\n",
       "   'fiber': 13.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 2,\n",
       "   'MinutesToNextMeal': 336,\n",
       "   'MinutesSincePrevMeal': 294},\n",
       "  {'timestamp': '2019-04-07 17:59:00',\n",
       "   'MealType': 'dinner',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 1079,\n",
       "   'HourOfDay': 17.983333333333334,\n",
       "   'calories': 1081.0,\n",
       "   'carbs': 124.0,\n",
       "   'protein': 47.0,\n",
       "   'fat': 36.0,\n",
       "   'fiber': 7.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 3,\n",
       "   'MinutesToNextMeal': 143,\n",
       "   'MinutesSincePrevMeal': 336},\n",
       "  {'timestamp': '2019-04-07 20:22:00',\n",
       "   'MealType': 'snack',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 1222,\n",
       "   'HourOfDay': 20.366666666666667,\n",
       "   'calories': 5.0,\n",
       "   'carbs': 0.0,\n",
       "   'protein': 0.0,\n",
       "   'fat': 0.0,\n",
       "   'fiber': 0.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 4,\n",
       "   'MinutesToNextMeal': -1,\n",
       "   'MinutesSincePrevMeal': 143}],\n",
       " [{'timestamp': '2022-02-20 07:57:00',\n",
       "   'MealType': 'breakfast',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 477,\n",
       "   'HourOfDay': 7.95,\n",
       "   'calories': 608.0,\n",
       "   'carbs': 66.0,\n",
       "   'protein': 66.0,\n",
       "   'fat': 10.5,\n",
       "   'fiber': 0.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 1,\n",
       "   'MinutesToNextMeal': 227,\n",
       "   'MinutesSincePrevMeal': -1},\n",
       "  {'timestamp': '2022-02-20 11:44:00',\n",
       "   'MealType': 'lunch',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 704,\n",
       "   'HourOfDay': 11.733333333333333,\n",
       "   'calories': 435.0,\n",
       "   'carbs': 16.0,\n",
       "   'protein': 66.0,\n",
       "   'fat': 14.0,\n",
       "   'fiber': 4.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 2,\n",
       "   'MinutesToNextMeal': 381,\n",
       "   'MinutesSincePrevMeal': 227},\n",
       "  {'timestamp': '2022-02-20 18:05:00',\n",
       "   'MealType': 'dinner',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 1085,\n",
       "   'HourOfDay': 18.083333333333332,\n",
       "   'calories': 655.0,\n",
       "   'carbs': 53.0,\n",
       "   'protein': 16.0,\n",
       "   'fat': 46.0,\n",
       "   'fiber': 10.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 3,\n",
       "   'MinutesToNextMeal': -1,\n",
       "   'MinutesSincePrevMeal': 381}],\n",
       " [{'timestamp': '2019-10-29 05:57:00',\n",
       "   'MealType': 'breakfast',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 357,\n",
       "   'HourOfDay': 5.95,\n",
       "   'calories': 448.0,\n",
       "   'carbs': 66.0,\n",
       "   'protein': 22.0,\n",
       "   'fat': 10.5,\n",
       "   'fiber': 0.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 1,\n",
       "   'MinutesToNextMeal': 344,\n",
       "   'MinutesSincePrevMeal': -1},\n",
       "  {'timestamp': '2019-10-29 11:41:00',\n",
       "   'MealType': 'lunch',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 701,\n",
       "   'HourOfDay': 11.683333333333334,\n",
       "   'calories': 575.0,\n",
       "   'carbs': 76.0,\n",
       "   'protein': 22.0,\n",
       "   'fat': 18.5,\n",
       "   'fiber': 11.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 2,\n",
       "   'MinutesToNextMeal': 355,\n",
       "   'MinutesSincePrevMeal': 344},\n",
       "  {'timestamp': '2019-10-29 17:36:00',\n",
       "   'MealType': 'dinner',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 1056,\n",
       "   'HourOfDay': 17.6,\n",
       "   'calories': 741.0,\n",
       "   'carbs': 135.0,\n",
       "   'protein': 42.0,\n",
       "   'fat': 7.0,\n",
       "   'fiber': 15.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 3,\n",
       "   'MinutesToNextMeal': -1,\n",
       "   'MinutesSincePrevMeal': 355}],\n",
       " [{'timestamp': '2019-03-30 12:13:00',\n",
       "   'MealType': 'lunch',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 733,\n",
       "   'HourOfDay': 12.216666666666667,\n",
       "   'calories': 1180.0,\n",
       "   'carbs': 81.0,\n",
       "   'protein': 88.0,\n",
       "   'fat': 54.5,\n",
       "   'fiber': 18.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 1,\n",
       "   'MinutesToNextMeal': 423,\n",
       "   'MinutesSincePrevMeal': -1},\n",
       "  {'timestamp': '2019-03-30 19:16:00',\n",
       "   'MealType': 'dinner',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 1156,\n",
       "   'HourOfDay': 19.266666666666666,\n",
       "   'calories': 495.0,\n",
       "   'carbs': 39.0,\n",
       "   'protein': 48.0,\n",
       "   'fat': 17.0,\n",
       "   'fiber': 6.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 2,\n",
       "   'MinutesToNextMeal': -1,\n",
       "   'MinutesSincePrevMeal': 423}],\n",
       " [{'timestamp': '2023-03-24 08:42:00',\n",
       "   'MealType': 'Breakfast',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 522,\n",
       "   'HourOfDay': 8.7,\n",
       "   'calories': 712.0,\n",
       "   'carbs': 66.0,\n",
       "   'protein': 22.0,\n",
       "   'fat': 42.0,\n",
       "   'fiber': 0.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 1,\n",
       "   'MinutesToNextMeal': 274,\n",
       "   'MinutesSincePrevMeal': -1},\n",
       "  {'timestamp': '2023-03-24 13:16:00',\n",
       "   'MealType': 'Lunch',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 796,\n",
       "   'HourOfDay': 13.266666666666667,\n",
       "   'calories': 555.0,\n",
       "   'carbs': 94.0,\n",
       "   'protein': 12.0,\n",
       "   'fat': 13.0,\n",
       "   'fiber': 5.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 2,\n",
       "   'MinutesToNextMeal': 215,\n",
       "   'MinutesSincePrevMeal': 274},\n",
       "  {'timestamp': '2023-03-24 16:51:00',\n",
       "   'MealType': 'dinner',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 1011,\n",
       "   'HourOfDay': 16.85,\n",
       "   'calories': 45.0,\n",
       "   'carbs': 11.0,\n",
       "   'protein': 1.0,\n",
       "   'fat': 1.0,\n",
       "   'fiber': 1.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 3,\n",
       "   'MinutesToNextMeal': 263,\n",
       "   'MinutesSincePrevMeal': 215},\n",
       "  {'timestamp': '2023-03-24 21:14:00',\n",
       "   'MealType': 'dinner',\n",
       "   'MealNumber': 2,\n",
       "   'MinuteOfDay': 1274,\n",
       "   'HourOfDay': 21.233333333333334,\n",
       "   'calories': 810.0,\n",
       "   'carbs': 100.0,\n",
       "   'protein': 38.0,\n",
       "   'fat': 41.0,\n",
       "   'fiber': 28.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 4,\n",
       "   'MinutesToNextMeal': -1,\n",
       "   'MinutesSincePrevMeal': 263}],\n",
       " [{'timestamp': '2024-05-19 08:56:00',\n",
       "   'MealType': 'Breakfast',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 536,\n",
       "   'HourOfDay': 8.933333333333334,\n",
       "   'calories': 268.0,\n",
       "   'carbs': 24.0,\n",
       "   'protein': 22.0,\n",
       "   'fat': 10.5,\n",
       "   'fiber': 0.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 1,\n",
       "   'MinutesToNextMeal': 209,\n",
       "   'MinutesSincePrevMeal': -1},\n",
       "  {'timestamp': '2024-05-19 12:25:00',\n",
       "   'MealType': 'Lunch',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 745,\n",
       "   'HourOfDay': 12.416666666666666,\n",
       "   'calories': 1110.0,\n",
       "   'carbs': 93.0,\n",
       "   'protein': 84.0,\n",
       "   'fat': 44.0,\n",
       "   'fiber': 4.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 2,\n",
       "   'MinutesToNextMeal': 201,\n",
       "   'MinutesSincePrevMeal': 209},\n",
       "  {'timestamp': '2024-05-19 15:46:00',\n",
       "   'MealType': 'Snacks',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 946,\n",
       "   'HourOfDay': 15.766666666666667,\n",
       "   'calories': 195.0,\n",
       "   'carbs': 42.0,\n",
       "   'protein': 5.0,\n",
       "   'fat': 2.0,\n",
       "   'fiber': 3.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 3,\n",
       "   'MinutesToNextMeal': 116,\n",
       "   'MinutesSincePrevMeal': 201},\n",
       "  {'timestamp': '2024-05-19 17:42:00',\n",
       "   'MealType': 'Dinner',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 1062,\n",
       "   'HourOfDay': 17.7,\n",
       "   'calories': 580.0,\n",
       "   'carbs': 47.0,\n",
       "   'protein': 45.0,\n",
       "   'fat': 25.0,\n",
       "   'fiber': 4.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 4,\n",
       "   'MinutesToNextMeal': 297,\n",
       "   'MinutesSincePrevMeal': 116},\n",
       "  {'timestamp': '2024-05-19 22:39:00',\n",
       "   'MealType': 'Dinner',\n",
       "   'MealNumber': 2,\n",
       "   'MinuteOfDay': 1359,\n",
       "   'HourOfDay': 22.65,\n",
       "   'calories': 140.0,\n",
       "   'carbs': 17.0,\n",
       "   'protein': 2.0,\n",
       "   'fat': 8.0,\n",
       "   'fiber': 0.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 5,\n",
       "   'MinutesToNextMeal': -1,\n",
       "   'MinutesSincePrevMeal': 297}],\n",
       " [{'timestamp': '2021-01-26 07:24:00',\n",
       "   'MealType': 'breakfast',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 444,\n",
       "   'HourOfDay': 7.4,\n",
       "   'calories': 448.0,\n",
       "   'carbs': 66.0,\n",
       "   'protein': 22.0,\n",
       "   'fat': 10.5,\n",
       "   'fiber': 0.0,\n",
       "   'has_image': False,\n",
       "   'MealSequence': 1,\n",
       "   'MinutesToNextMeal': 264,\n",
       "   'MinutesSincePrevMeal': -1},\n",
       "  {'timestamp': '2021-01-26 11:48:00',\n",
       "   'MealType': 'dinner',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 708,\n",
       "   'HourOfDay': 11.8,\n",
       "   'calories': 114.0,\n",
       "   'carbs': 4.0,\n",
       "   'protein': 8.0,\n",
       "   'fat': 7.0,\n",
       "   'fiber': 1.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 2,\n",
       "   'MinutesToNextMeal': 450,\n",
       "   'MinutesSincePrevMeal': 264},\n",
       "  {'timestamp': '2021-01-26 19:18:00',\n",
       "   'MealType': 'lunch',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 1158,\n",
       "   'HourOfDay': 19.3,\n",
       "   'calories': 830.0,\n",
       "   'carbs': 92.0,\n",
       "   'protein': 17.0,\n",
       "   'fat': 42.0,\n",
       "   'fiber': 10.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 3,\n",
       "   'MinutesToNextMeal': -1,\n",
       "   'MinutesSincePrevMeal': 450}],\n",
       " [{'timestamp': '2019-03-16 08:01:00',\n",
       "   'MealType': 'breakfast',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 481,\n",
       "   'HourOfDay': 8.016666666666667,\n",
       "   'calories': 268.0,\n",
       "   'carbs': 24.0,\n",
       "   'protein': 22.0,\n",
       "   'fat': 10.5,\n",
       "   'fiber': 0.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 1,\n",
       "   'MinutesToNextMeal': 253,\n",
       "   'MinutesSincePrevMeal': -1},\n",
       "  {'timestamp': '2019-03-16 12:14:00',\n",
       "   'MealType': 'lunch',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 734,\n",
       "   'HourOfDay': 12.233333333333333,\n",
       "   'calories': 725.0,\n",
       "   'carbs': 94.0,\n",
       "   'protein': 44.0,\n",
       "   'fat': 20.0,\n",
       "   'fiber': 4.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 2,\n",
       "   'MinutesToNextMeal': 358,\n",
       "   'MinutesSincePrevMeal': 253},\n",
       "  {'timestamp': '2019-03-16 18:12:00',\n",
       "   'MealType': 'dinner',\n",
       "   'MealNumber': 1,\n",
       "   'MinuteOfDay': 1092,\n",
       "   'HourOfDay': 18.2,\n",
       "   'calories': 915.0,\n",
       "   'carbs': 117.0,\n",
       "   'protein': 33.0,\n",
       "   'fat': 36.0,\n",
       "   'fiber': 6.0,\n",
       "   'has_image': True,\n",
       "   'MealSequence': 3,\n",
       "   'MinutesToNextMeal': -1,\n",
       "   'MinutesSincePrevMeal': 358}]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['nutrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgm_dinner",
   "language": "python",
   "name": "cgm_dinner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
